{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade de Programação - Qual o próximo hit do Spotify?\n",
    "## Instruções\n",
    "\n",
    "Imagine que você foi contratado por uma das maiores gravadoras do mundo. Sua missão? Desenvolver um modelo preditivo que antecipe o próximo grande sucesso musical no Spotify. A gravadora, que já lançou grandes artistas e hits globais, está buscando maneiras de otimizar seus investimentos e estratégias de marketing, apostando nos talentos certos e maximizando o impacto das suas músicas. Para isso, eles precisam de um modelo robusto e preciso, capaz de prever a popularidade de uma canção antes mesmo de ela ser lançada.\n",
    "\n",
    "Você receberá uma base de dados de mais de 100 mil músicas, cada uma contendo uma série de características acústicas, metadados, e informações de popularidade extraídas diretamente do Spotify. Com esses dados, a gravadora espera que você entregue insights valiosos e um modelo preditivo que possa ser integrado às suas operações.\n",
    "\n",
    "**Objetivo**\n",
    "\n",
    "Seu objetivo é criar um modelo de machine learning que preveja a popularidade de uma música no Spotify. Este modelo ajudará a gravadora a tomar decisões mais informadas sobre onde investir seus recursos, quais artistas promover e quais faixas têm maior potencial de se tornarem virais.\n",
    "\n",
    "**Dados Disponíveis**\n",
    "\n",
    "Você terá acesso a um rico conjunto de dados com mais de 100 mil músicas do Spotify. Essas músicas vêm com uma variedade de features, incluindo características acústicas (como energia, valência, tempo, etc.), metadados (como gênero, ano de lançamento, etc.) e um índice de popularidade medido diretamente pelo Spotify.\n",
    "\n",
    "**Entregáveis**\n",
    "\n",
    "- Notebook Completo: Um notebook Jupyter documentando todo o processo, desde a exploração dos dados até a criação e avaliação do modelo.\n",
    "- Arquivo CSV de resultados: Submeta os resultados em csv do seu melhor modelo treinado, conforme template disponibilizado.\n",
    "\n",
    "Seja o arquiteto do próximo grande hit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importação das Bibliotecas\n",
    "\n",
    "O primeiro passo para a construção desse modelo preditivo consiste na importação das bibliotecas, que é um processo responsável por carregar funcionalidades de um pacote externo para uso no projeto. As bibliotecas contêm módulos com funções, classes e métodos que facilitam várias tarefas (como manipulação de dados, visualização e modelagem). Dessa forma, as bibliotecas utilizadas nesse notebook consistiu em:\n",
    "\n",
    "1. **pandas as pd:** O pandas é uma biblioteca usada para manipular e analisar dados em Python, fornecendo estruturas de dados como DataFrame e Series, para carregar dados, limpar, explorar e transformá-los para modelagem.\n",
    "\n",
    "2. **numpy as np:** O numpy é uma biblioteca para computação numérica que fornece suporte para arrays multidimensionais e operações matemáticas, sendo utilizado em conjunto com pandas para cálculos numéricos ou manipulação de arrays.\n",
    "\n",
    "3. **OneHotEncoder do sklearn.preprocessing:** O OneHotEncoder é usado para transformar variáveis categóricas em vetores binários (one-hot encoding), tornando-as adequadas para uso em modelos de machine learning. Cada categoria é transformada em uma coluna separada, preenchida com 1 ou 0 para indicar a presença da categoria.\n",
    "\n",
    "4. **matplotlib.pyplot as plt:** O matplotlib é uma biblioteca para a criação de gráficos e visualizações em Python, como histogramas, dispersões, e heatmaps, facilitando a análise visual de dados.\n",
    "\n",
    "5. **seaborn as sns:** O seaborn é uma biblioteca de visualização baseada no matplotlib que fornece funções para criar gráficos estatísticos, permitindo a criação de gráficos mais estilizados e interpretações visuais mais detalhadas para análise de dados, como correlações e distribuições.\n",
    "\n",
    "6. **train_test_split do sklearn.model_selection:** O train_test_split é usado para dividir os dados em conjuntos de treino e validação (ou teste), ajudando a avaliar o desempenho do modelo de forma imparcial, garantindo que o modelo não seja avaliado nos mesmos dados em que foi treinado.\n",
    "\n",
    "7. **RandomForestClassifier do sklearn.ensemble:** O RandomForestClassifier é um modelo de aprendizado de máquina que utiliza uma combinação de várias árvores de decisão para melhorar a precisão da classificação, sendo um método para tarefas de classificação e regressão.\n",
    "\n",
    "8. **accuracy_score, precision_score, recall_score, f1_score, classification_report do sklearn.metrics:** são funções usadas para avaliar o desempenho do modelo:\n",
    "     - **accuracy_score:** Mede a acurácia geral do modelo (proporção de previsões corretas).\n",
    "     - **precision_score:** Mede a precisão (proporção de previsões positivas corretas).\n",
    "     - **recall_score:** Mede a sensibilidade ou recall (proporção de positivos reais corretamente identificados).\n",
    "     - **f1_score:** Combinação de precisão e recall (métrica balanceada).\n",
    "     - **classification_report:** Fornece um relatório completo das métricas de classificação (precisão, recall, F1-score) por classe.\n",
    "\n",
    "9. **GridSearchCV do sklearn.model_selection:** O GridSearchCV é utilizado para otimizar hiperparâmetros de modelos de machine learning, realizando uma busca em uma grade de combinações de parâmetros, treinando e avaliando o modelo para encontrar a melhor combinação possível.\n",
    "\n",
    "Sendo assim, foram feita a importação de todas essas bibliotecas, conforme consta na célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Carregamento do Dataset\n",
    "\n",
    "**Objetivo da Célula:**\n",
    "\n",
    "Esta célula faz a análise inicial do dataset de treinamento carregando o arquivo CSV e visualizar as primeiras 5 entradas do dataset.\n",
    "\n",
    "**Passo a Passo do Código:**\n",
    "\n",
    "1. **Carregar o Dataset de Treinamento:**\n",
    "   - O arquivo `train.csv` é carregado em um DataFrame para ``análise``.\n",
    "   - O método `.head()` é utilizado para exibir as primeiras linhas, fornecendo uma visão geral das variáveis e seus tipos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o arquivo de treinamento para inspeção inicial\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Exibindo as primeiras linhas do dataset para análise inicial\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Análise dos Dados\n",
    "\n",
    "**Objetivo da Célula:**\n",
    "O objetivo é verificar a estrutura dos dados e realizar uma análise preliminar para identificar possíveis problemas, como valores nulos ou inconsistências.\n",
    "\n",
    "**Passo a Passo do Código:**\n",
    "\n",
    "1. **Verificar Valores Nulos:**\n",
    "   - O método `.isnull().sum()` é utilizado para contar valores nulos em cada coluna.\n",
    "   - Exibe apenas as colunas com valores nulos, caso existam, para identificação de possíveis problemas.\n",
    "\n",
    "2. **Analisar Estatísticas Descritivas:**\n",
    "   - O método `.describe()` gera estatísticas descritivas (média, desvio padrão, mínimo, máximo) para as variáveis numéricas, ajudando a entender a distribuição dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando valores nulos no dataset\n",
    "missing_values = train_data.isnull().sum()\n",
    "\n",
    "# Exibindo colunas com valores nulos (se houver)\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Analisando estatísticas descritivas para variáveis numéricas\n",
    "descriptive_stats = train_data.describe()\n",
    "\n",
    "descriptive_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Análise de Distribuição de Variáveis Categóricas\n",
    "\n",
    "**Objetivo da Célula:**\n",
    "Verificar a distribuição das variáveis categóricas para entender suas frequências e identificar possíveis desequilíbrios ou padrões importantes.\n",
    "\n",
    "**Passo a Passo do Código:**\n",
    "\n",
    "1. **Identificar Variáveis Categóricas:**\n",
    "   - São selecionadas as variáveis categóricas de interesse para análise: `explicit`, `key`, `mode`, `time_signature`, `track_genre`.\n",
    "\n",
    "2. **Calcular Distribuição:**\n",
    "   - Para cada variável categórica, é calculada a distribuição de frequências (contagem de valores).\n",
    "\n",
    "3. **Visualizar Distribuição:**\n",
    "   - As distribuições são exibidas para cada variável categórica, ajudando a identificar padrões ou valores dominantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando a distribuição das variáveis categóricas\n",
    "categorical_columns = ['explicit', 'key', 'mode', 'time_signature', 'track_genre']\n",
    "\n",
    "# Análise de distribuição para variáveis categóricas\n",
    "categorical_distributions = {}\n",
    "for col in categorical_columns:\n",
    "    categorical_distributions[col] = train_data[col].value_counts()\n",
    "\n",
    "# Exibindo a distribuição das variáveis categóricas\n",
    "categorical_distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Detecção e Tratamento de Outliers\n",
    "\n",
    "**Objetivo da Célula:**\n",
    "Identificar e tratar possíveis outliers em variáveis numéricas utilizando o método do IQR (Intervalo Interquartil), garantindo que dados atípicos não prejudiquem a modelagem.\n",
    "\n",
    "**Passo a Passo do Código:**\n",
    "\n",
    "1. **Função para Detecção de Outliers:**\n",
    "   - Função `detect_outliers()` é definida para identificar valores que estão além de 1.5 vezes o IQR, calculando limites inferiores e superiores para cada variável.\n",
    "\n",
    "2. **Detecção de Outliers:**\n",
    "   - Possíveis outliers são detectados nas colunas `duration_ms` e `loudness`.\n",
    "\n",
    "3. **Remoção de Outliers:**\n",
    "   - Os outliers identificados são removidos do dataset original, e o tamanho do dataset é verificado após a limpeza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo uma função para identificar possíveis outliers usando o método do IQR\n",
    "def detect_outliers(column):\n",
    "    Q1 = np.percentile(column, 25)\n",
    "    Q3 = np.percentile(column, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return column[(column < lower_bound) | (column > upper_bound)]\n",
    "\n",
    "# Detectando possíveis outliers para variáveis numéricas selecionadas\n",
    "outliers_duration = detect_outliers(train_data['duration_ms'])\n",
    "outliers_loudness = detect_outliers(train_data['loudness'])\n",
    "\n",
    "# Exibindo contagem de possíveis outliers\n",
    "len(outliers_duration), len(outliers_loudness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo outliers das colunas 'duration_ms' e 'loudness'\n",
    "train_data_cleaned = train_data[~train_data['duration_ms'].isin(outliers_duration)]\n",
    "train_data_cleaned = train_data_cleaned[~train_data_cleaned['loudness'].isin(outliers_loudness)]\n",
    "\n",
    "# Verificando o tamanho do dataset após remoção dos outliers\n",
    "train_data_cleaned.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Codificação de Variáveis Categóricas com One-Hot Encoding\n",
    "\n",
    "**Objetivo da Célula:**\n",
    "Transformar variáveis categóricas em variáveis numéricas através de One-Hot Encoding, preparando os dados para serem utilizados por modelos de machine learning.\n",
    "\n",
    "**Passo a Passo do Código:**\n",
    "\n",
    "1. **Selecionar Variáveis Categóricas para Codificação:**\n",
    "   - As variáveis categóricas previamente identificadas são selecionadas para codificação.\n",
    "\n",
    "2. **Aplicar One-Hot Encoding:**\n",
    "   - `OneHotEncoder` do Scikit-learn é utilizado para criar variáveis dummy para cada categoria.\n",
    "   - O resultado é convertido em um DataFrame e concatenado ao dataset original, removendo as variáveis categóricas originais.\n",
    "\n",
    "3. **Verificar Dataset Final:**\n",
    "   - O tamanho do dataset final é verificado e suas primeiras linhas são exibidas para confirmar a codificação correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando One-Hot Encoding para variáveis categóricas\n",
    "# Selecionando as colunas para codificação\n",
    "categorical_features = ['explicit', 'key', 'mode', 'time_signature', 'track_genre']\n",
    "\n",
    "# Criando uma instância do OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Aplicando a codificação e convertendo para um DataFrame\n",
    "encoded_features = encoder.fit_transform(train_data_cleaned[categorical_features])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# Combinando as colunas codificadas ao dataset original (removendo as colunas categóricas originais)\n",
    "train_data_final = pd.concat([train_data_cleaned.drop(categorical_features, axis=1).reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Verificando o tamanho e as primeiras linhas do dataset final\n",
    "train_data_final.shape, train_data_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Formulação de Hipóteses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hipótese 1: Músicas com Maior \"Valence\" Tendem a Ser Mais Populares\n",
    "\n",
    "**Explicação da Célula:**\n",
    "\n",
    "A hipótese testada é que músicas com maior \"valence\" (positividade musical) são mais populares. A variável \"valence\" varia de 0.0 (faixa mais negativa) a 1.0 (faixa mais positiva), e a suposição é de que músicas mais \"felizes\" tendem a atrair um público maior e, portanto, são mais populares.\n",
    "\n",
    "**Visualização:**\n",
    "\n",
    "Para testar essa hipótese, foi criado um boxplot para comparar a distribuição de \"valence\" entre músicas populares (`popularity_target = 1`) e não populares (`popularity_target = 0`). O gráfico permite observar a mediana, o intervalo interquartil e possíveis outliers para cada grupo.\n",
    "\n",
    "**Análise do Gráfico:**\n",
    "\n",
    "- A distribuição de \"valence\" para músicas populares e não populares é bastante similar.\n",
    "- **Mediana:** A mediana da \"valence\" é ligeiramente menor para músicas populares, o que indica que, em média, elas têm um tom um pouco menos positivo do que as músicas não populares.\n",
    "- **Intervalo Interquartil:** O intervalo interquartil (IQR), que representa a faixa central de 50% dos dados, é bastante semelhante para ambos os grupos, indicando que a maioria das músicas tem níveis de \"valence\" próximos, independentemente de serem populares ou não.\n",
    "- **Outliers:** Não há uma presença significativa de outliers, e a amplitude geral dos valores de \"valence\" é semelhante em ambos os grupos.\n",
    "\n",
    "**Conclusão da Análise:**\n",
    "\n",
    "A visualização não evidencia uma diferença clara entre músicas populares e não populares com relação à variável \"valence\". Isso sugere que, embora a positividade de uma faixa possa ter algum impacto na popularidade, outros fatores também desempenham um papel importante, ou essa variável isolada não é um forte indicador da popularidade.\n",
    "\n",
    "O resultado desse boxplot não suporta completamente a hipótese inicial de que músicas mais positivas (com maior \"valence\") são significativamente mais populares, uma vez que as distribuições para os dois grupos são muito similares. Portanto, para uma análise mais conclusiva, é importante considerar outras variáveis e fatores que possam influenciar a popularidade de uma música."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Criar uma visualização para comparar valence com popularidade\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='popularity_target', y='valence', data=train_data_cleaned)\n",
    "plt.title('Distribuição de Valence para Músicas Populares vs Não Populares')\n",
    "plt.xlabel('Popularidade (0 = Não Popular, 1 = Popular)')\n",
    "plt.ylabel('Valence')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hipótese 2: Músicas com Alta \"Danceability\" Têm Maior Chance de Serem Populares\n",
    "\n",
    "**Explicação da Célula:**\n",
    "\n",
    "A hipótese é que músicas com alta \"danceability\", ou seja, músicas que são mais fáceis de dançar (valores próximos de 1.0), têm uma maior chance de serem populares. Isso ocorre porque essas músicas podem ter maior apelo para ambientes sociais, como festas, clubes ou eventos, tornando-as mais suscetíveis a ganhar popularidade.\n",
    "\n",
    "**Visualização:**\n",
    "\n",
    "Foi criado um boxplot para comparar a distribuição da variável \"danceability\" entre músicas populares (`popularity_target = 1`) e não populares (`popularity_target = 0`). Este gráfico é útil para observar a mediana, intervalo interquartil, e outliers para os dois grupos, permitindo identificar diferenças na distribuição de \"danceability\".\n",
    "\n",
    "**Análise do Gráfico:**\n",
    "\n",
    "- **Mediana Similar:** A mediana de \"danceability\" é muito próxima para ambos os grupos de músicas populares e não populares, indicando que a tendência central de quão dançável é uma música não difere significativamente entre os grupos.\n",
    "- **Intervalo Interquartil (IQR):** O IQR de \"danceability\" é ligeiramente mais alto para músicas populares, sugerindo que músicas populares têm uma distribuição um pouco mais ampla de valores de \"danceability\" quando comparadas a músicas não populares.\n",
    "- **Outliers:** Em ambos os grupos, há a presença de outliers com valores muito baixos de \"danceability\". Isso indica que existem algumas músicas que são pouco dançáveis, independentemente de serem populares ou não.\n",
    "- **Distribuição Semelhante:** No geral, a distribuição de \"danceability\" é similar para os dois grupos, com uma leve sugestão de que músicas populares têm uma ligeira tendência a serem mais \"dançáveis\" que as não populares.\n",
    "\n",
    "**Conclusão da Análise:**\n",
    "\n",
    "A visualização não fornece uma evidência forte para confirmar que músicas mais dançantes têm maior probabilidade de serem populares, embora haja uma leve tendência nesse sentido. A similaridade entre as distribuições de \"danceability\" para músicas populares e não populares sugere que, isoladamente, essa variável não é um forte preditor de popularidade. Portanto, é provável que outros fatores, além de \"danceability\", desempenhem um papel mais significativo na determinação da popularidade de uma música. \n",
    "\n",
    "Assim, a hipótese de que músicas mais dançáveis têm uma maior chance de serem populares não é refutada, mas não se apresenta como uma conclusão sólida com base apenas nesta análise visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma visualização para comparar danceability com popularidade\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='popularity_target', y='danceability', data=train_data_cleaned)\n",
    "plt.title('Distribuição de Danceability para Músicas Populares vs Não Populares')\n",
    "plt.xlabel('Popularidade (0 = Não Popular, 1 = Popular)')\n",
    "plt.ylabel('Danceability')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hipótese 3: Músicas Explícitas Podem Ter Menos Chances de Serem Populares\n",
    "\n",
    "**Explicação da Célula:**\n",
    "\n",
    "A hipótese é que músicas com conteúdo explícito têm uma menor chance de se tornarem populares devido às possíveis restrições de exposição em plataformas de streaming ou rádio. Essas músicas podem ser menos acessíveis para certos públicos ou países, afetando sua capacidade de alcançar popularidade.\n",
    "\n",
    "**Visualização:**\n",
    "\n",
    "Um gráfico de barras foi criado para comparar a contagem de músicas explícitas (`True`) e não explícitas (`False`) para os dois grupos de popularidade (`popularity_target = 0` para músicas não populares e `popularity_target = 1` para músicas populares). O gráfico destaca a proporção de músicas explícitas e não explícitas dentro de cada grupo de popularidade.\n",
    "\n",
    "**Análise do Gráfico:**\n",
    "\n",
    "- **Predominância de Músicas Não Explícitas:** Tanto no grupo de músicas populares quanto no de não populares, a maioria das músicas não possui conteúdo explícito (barras azuis). Isso indica que músicas explícitas representam uma proporção relativamente pequena do total, independentemente do nível de popularidade.\n",
    "- **Proporção Similar de Músicas Explícitas:** A proporção de músicas explícitas (barras laranjas) é similar para ambos os grupos de popularidade. Isso sugere que o fato de uma música ser explícita ou não explícita não influencia de forma significativa se ela será popular ou não.\n",
    "- **Baixa Contagem Absoluta de Músicas Explícitas:** Músicas explícitas têm uma contagem absoluta menor em relação às não explícitas em ambos os grupos, indicando que músicas com conteúdo explícito são, em geral, menos comuns no dataset analisado.\n",
    "\n",
    "**Conclusão da Análise:**\n",
    "\n",
    "A visualização não fornece uma evidência forte para apoiar a hipótese de que músicas explícitas têm menos chances de serem populares. A proporção de músicas explícitas é relativamente baixa para ambos os grupos (populares e não populares), e a similaridade entre os dois grupos sugere que a presença de conteúdo explícito por si só não é um fator determinante para a popularidade de uma música.\n",
    "\n",
    "No entanto, a baixa contagem de músicas explícitas no dataset pode ser uma possível razão para essa falta de evidência. Assim, pode ser interessante explorar outros fatores em conjunto com a presença de conteúdo explícito para entender melhor o impacto na popularidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma visualização para comparar músicas explícitas com popularidade\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='popularity_target', hue='explicit', data=train_data_cleaned)\n",
    "plt.title('Distribuição de Músicas Explícitas vs Não Explícitas para Músicas Populares')\n",
    "plt.xlabel('Popularidade (0 = Não Popular, 1 = Popular)')\n",
    "plt.ylabel('Contagem')\n",
    "plt.legend(title='Explícita')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Análise de Correlação para Seleção de Features\n",
    "\n",
    "**Objetivo da Célula:**\n",
    "Calcular e visualizar a correlação entre variáveis numéricas e a variável-alvo `popularity_target`, a fim de identificar as features mais relevantes para a modelagem.\n",
    "\n",
    "**Passo a Passo do Código:**\n",
    "\n",
    "1. **Calcular Matriz de Correlação:**\n",
    "   - A matriz de correlação é calculada para todas as variáveis numéricas.\n",
    "\n",
    "2. **Visualizar Correlação com a Variável-Alvo:**\n",
    "   - Um heatmap é gerado para visualizar as correlações das variáveis numéricas com a variável `popularity_target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a matriz de correlação para variáveis numéricas\n",
    "numerical_columns = train_data_cleaned.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "numerical_columns.remove('popularity_target')  # Excluindo a coluna-alvo para análise separada\n",
    "corr_matrix = train_data_cleaned[numerical_columns + ['popularity_target']].corr()\n",
    "\n",
    "# Plotando um heatmap para visualizar correlações com a variável-alvo 'popularity_target'\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix[['popularity_target']].sort_values(by='popularity_target', ascending=False), \n",
    "            annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlação das variáveis numéricas com Popularity Target')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Seleção de Features para Modelagem\n",
    "\n",
    "**Objetivo da Célula:**\n",
    "Selecionar as features mais relevantes para a modelagem, garantindo que o modelo seja treinado com variáveis que têm maior impacto na previsão da popularidade.\n",
    "\n",
    "**Passo a Passo do Código:**\n",
    "\n",
    "1. **Selecionar Features Mais Relevantes:**\n",
    "   - Com base na análise de correlação e entendimento dos dados, as features mais relevantes são selecionadas, incluindo variáveis dummy de gêneros.\n",
    "\n",
    "2. **Extrair Features e Variável-Alvo:**\n",
    "   - As variáveis independentes (`X`) e a variável-alvo (`y`) são separadas para preparação do modelo.\n",
    "\n",
    "3. **Dividir Dataset em Treino e Teste:**\n",
    "   - O dataset é dividido em 80% para treino e 20% para validação, permitindo avaliar a performance do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando as features mais relevantes para o modelo\n",
    "selected_features = [\n",
    "    'valence', 'danceability', 'energy', 'instrumentalness', 'loudness', \n",
    "    'explicit_True', 'explicit_False', 'mode_0', 'mode_1', 'key_0', 'key_1',\n",
    "    'key_2', 'key_3', 'key_4', 'key_5', 'key_6', 'key_7', 'key_8', 'key_9',\n",
    "    'key_10', 'key_11', 'time_signature_3', 'time_signature_4', 'time_signature_5'\n",
    "]\n",
    "\n",
    "# Adicionando as variáveis dummy de gênero\n",
    "genre_columns = [col for col in train_data_final.columns if col.startswith('track_genre')]\n",
    "selected_features.extend(genre_columns)\n",
    "\n",
    "# Extraindo as features e a variável alvo\n",
    "X = train_data_final[selected_features]\n",
    "y = train_data_final['popularity_target']\n",
    "\n",
    "# Dividindo o dataset em treino e teste (80% treino, 20% teste)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_val.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Treinamento de Modelo com Random Forest\n",
    "\n",
    "**Objetivo da Célula:**\n",
    "Treinar um modelo de Random Forest para prever a popularidade, avaliando sua performance através de métricas como acurácia, precisão, recall, e F1-score.\n",
    "\n",
    "**Passo a Passo do Código:**\n",
    "\n",
    "1. **Inicializar o Modelo:**\n",
    "   - `RandomForestClassifier` é inicializado com um `random_state` para garantir reprodutibilidade.\n",
    "\n",
    "2. **Treinar o Modelo:**\n",
    "   - O modelo é treinado com os dados de treino (`X_train`, `y_train`).\n",
    "\n",
    "3. **Fazer Previsões e Avaliar Desempenho:**\n",
    "   - O modelo faz previsões no conjunto de validação, e métricas de performance são calculadas e exibidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o modelo Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Treinando o modelo com os dados de treino\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões no conjunto de teste\n",
    "y_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Avaliando o desempenho do modelo\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "classification_rep = classification_report(y_val, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Ajuste Fino de Hiperparâmetros com GridSearchCV\n",
    "\n",
    "**Objetivo da Célula:**\n",
    "Melhorar a performance do modelo Random Forest ajustando seus hiperparâmetros através de uma busca em grade (GridSearchCV), a fim de encontrar a combinação ideal de parâmetros que maximize a acurácia.\n",
    "\n",
    "**Passo a Passo do Código:**\n",
    "\n",
    "1. **Definir Modelo Base:**\n",
    "   - O modelo Random Forest (`RandomForestClassifier`) é definido com um `random_state` para garantir reprodutibilidade.\n",
    "\n",
    "2. **Definir Espaço de Busca para Hiperparâmetros:**\n",
    "   - É configurado um dicionário de parâmetros (`param_grid`) para explorar diferentes combinações:\n",
    "     - `n_estimators`: Número de árvores na floresta (100, 200, 300).\n",
    "     - `max_depth`: Profundidade máxima das árvores (10, 20, sem limite).\n",
    "     - `min_samples_split`: Número mínimo de amostras necessárias para dividir um nó (2, 5, 10).\n",
    "\n",
    "3. **Configurar GridSearchCV:**\n",
    "   - `GridSearchCV` é utilizado para testar todas as combinações possíveis de hiperparâmetros, realizando uma validação cruzada (`cv=3`) para cada combinação e avaliando a performance usando acurácia (`scoring='accuracy'`).\n",
    "   - `n_jobs=-1` é usado para paralelizar a busca e acelerar o processo.\n",
    "\n",
    "4. **Treinar GridSearchCV:**\n",
    "   - A busca em grade é treinada com os dados de treino (`X_train`, `y_train`), testando cada combinação de hiperparâmetros.\n",
    "\n",
    "5. **Obter Melhor Conjunto de Hiperparâmetros:**\n",
    "   - A melhor combinação de hiperparâmetros encontrada é exibida pelo método `.best_params_`.\n",
    "\n",
    "6. **Avaliar Modelo Otimizado:**\n",
    "   - O melhor modelo (`best_model`) encontrado pela busca em grade é avaliado no conjunto de validação (`X_val`, `y_val`).\n",
    "   - A acurácia após o ajuste fino é calculada e exibida para verificar a melhoria de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o modelo base\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Definindo o espaço de busca dos hiperparâmetros\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Configurando o GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Treinando o modelo\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Melhor conjunto de hiperparâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores hiperparâmetros:\", best_params)\n",
    "\n",
    "# Avaliando o modelo com os melhores hiperparâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "accuracy = best_model.score(X_val, y_val)\n",
    "print(\"Acurácia após ajuste fino:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Transformação e Previsões no Conjunto de Teste\n",
    "\n",
    "**Objetivo da Célula:**\n",
    "Aplicar o modelo treinado e otimizado no conjunto de teste para fazer previsões finais e preparar o arquivo de submissão.\n",
    "\n",
    "**Passo a Passo do Código:**\n",
    "\n",
    "1. **Carregar Conjunto de Teste e Transformar Dados:**\n",
    "   - O conjunto de teste é carregado, e o One-Hot Encoding treinado é aplicado.\n",
    "\n",
    "2. **Alinhar Colunas com Conjunto de Treinamento:**\n",
    "   - O conjunto de teste é ajustado para garantir que suas colunas estejam alinhadas com o conjunto de treino.\n",
    "\n",
    "3. **Fazer Previsões e Criar Arquivo de Submissão:**\n",
    "   - O modelo treinado faz previsões no conjunto de teste, e os resultados são salvos em um arquivo CSV para submissão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o conjunto de teste\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Aplicar a transformação do One-Hot Encoding já treinado no conjunto de treino\n",
    "encoded_test_features = encoder.transform(test_data[categorical_features])\n",
    "encoded_test_df = pd.DataFrame(encoded_test_features, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# Combinando as colunas codificadas ao dataset original (removendo as colunas categóricas originais)\n",
    "test_data_final = pd.concat([test_data.drop(categorical_features, axis=1).reset_index(drop=True), encoded_test_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Garantir que as colunas do conjunto de teste estejam alinhadas com as do conjunto de treino\n",
    "missing_cols = set(X_train.columns) - set(test_data_final.columns)\n",
    "for col in missing_cols:\n",
    "    test_data_final[col] = 0  # Adiciona colunas ausentes no teste com valor 0\n",
    "\n",
    "test_data_final = test_data_final[X_train.columns]  # Reordenar para alinhar com X_train\n",
    "\n",
    "# Fazer as previsões usando o modelo treinado\n",
    "test_predictions = best_model.predict(test_data_final)\n",
    "\n",
    "# Criar o DataFrame de submissão com apenas as colunas necessárias\n",
    "submission = pd.DataFrame({\n",
    "    'track_unique_id': test_data['track_unique_id'],\n",
    "    'popularity_target': test_predictions\n",
    "})\n",
    "\n",
    "# Salvar o arquivo .csv\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
